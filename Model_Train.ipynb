{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrXhUXSSmxxmwo/UPw7Kwn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ9dPwkTtKwi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report, roc_auc_score, r2_score, confusion_matrix, roc_curve, auc, precision_recall_curve\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.calibration import calibration_curve\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold # Import StratifiedKFold\n","from sklearn.feature_selection import SelectFromModel\n","import pickle\n","\n","# Load your dataset (update the file path as needed)\n","df = pd.read_csv('/content/drive/MyDrive/la_hai_la_kei_hola_jasto_xa_hai.csv')\n","\n","# Convert date column to datetime\n","df['acq_date'] = pd.to_datetime(df['acq_date'])\n","df = df.sort_values(by='acq_date')\n","\n","# **Preprocessing**\n","print(\"Missing values per column:\")\n","print(df.isnull().sum())\n","\n","# Define target variable\n","df['fire_risk'] = ((df['frp'] > 0).astype(int))\n","\n","X = df.drop(columns=['fire_risk'])  # Features\n","y = df['fire_risk']  # Target\n","\n","rus = RandomUnderSampler(sampling_strategy=0.8, random_state=42)  # 1 fire : 2 no-fire ratio\n","X_resampled, y_resampled = rus.fit_resample(X, y)\n","\n","df = X_resampled.assign(fire_risk=y_resampled)\n","\n","# Separate the majority (0) and minority (1) classes\n","df_fire = (df['fire_risk'] == 1).sum()\n","df_no_fire = (df['fire_risk'] == 0).sum()\n","print(f\"Number of fire instances: {df_fire}, Number of no-fire instances: {df_no_fire}\")\n","\n","# Critical new features\n","df['fuel_moisture_index'] = (df['swvl1_mean'] / 100) * df['t2m_mean']\n","df['flammability_score'] = (\n","    df['vpd'] * df['wind_speed'] / (df['swvl4_mean'] + 1e-6)\n",")\n","df['persistent_drought'] = df[['7d_drought','14d_drought']].mean(axis=1)\n","\n","df = df.dropna()\n","\n","drop_cols = ['latitude', 'longitude', 'fire_count', 'acq_date', 'fire_risk', 'confidence', 'frp', 'brightness','7d_max_d2m', '3d_max_d2m',\n","             'date', 'cell_id', 'total_frp', 'number', 'latitude_x', 'longitude_x', 't2m_mean', '7d_avg_d2m', '7d_avg_d2m', 'swvl3_mean',\n","             'nearest_lon', '3d_avg_wind', '7d_avg_wind', '14d_avg_wind', '30d_avg_wind','wind_speed', 'swvl1_mean_7d_avg', 'tp_sum','swvl3_mean_14d_avg',\n","             'valid_time', 'longitude_y', 'latitude_y', 'wind_temp_interaction','swvl3_mean_7d_avg','swvl2_mean_3d_avg', 'swvl2_mean_7d_avg',\n","             'date_7d_before', 'nearest_lat', 'geometry', 'centroid_lat', 'centroid_lon', 'grid_id','swvl1_mean_3d_avg', 'swvl1_mean', 'swvl1_mean_14d_avg'\n","             ]\n","\n","X = df.drop(columns=drop_cols, errors='ignore')\n","y = df['fire_risk']\n","\n","# Random 80-20 split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","print(\"Features in X_train (before selection):\", X_train.columns.tolist())\n","\n","# Convert object columns to numeric (essential before scaling or initial model training)\n","for col in X_train.columns:\n","    if X_train[col].dtype == 'object':\n","        X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n","for col in X_test.columns:\n","    if X_test[col].dtype == 'object':\n","        X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n","\n","# --- NEW SECTION: Initial Model Training and Feature Selection ---\n","\n","# We need an initial model to get feature importances.\n","# Use the same best parameters you found previously for this initial model.\n","initial_model_params = {\n","    'scale_pos_weight': 4.5,\n","    'learning_rate': 0.1,\n","    'max_depth': 7,\n","    'subsample': 0.7,\n","    'colsample_bytree': 0.9,\n","    'reg_alpha': 1,\n","    'reg_lambda': 2,\n","    'gamma': 1,\n","    'eval_metric': ['auc', 'aucpr', 'logloss'],\n","    'n_estimators': 600,\n","    'tree_method': 'hist',\n","    'early_stopping_rounds': 50,\n","    'enable_categorical': False,\n","    'use_label_encoder': False,\n","    'n_jobs': -1\n","}\n","# params = {\n","#     'scale_pos_weight': 4.5,  # ~4.57\n","#     'learning_rate': 0.1,\n","#     'max_depth': 5,\n","#     'subsample': 0.7,\n","#     'colsample_bytree': 0.6,\n","#     'reg_alpha': 0.5,\n","#     'reg_lambda': 0.8,\n","#     'gamma': 0.2,\n","#     'eval_metric': ['auc', 'aucpr', 'logloss']  # Focus on precision-recall AUC\n","# }\n","\n","print(\"\\n--- Training initial model for Feature Importance selection ---\")\n","initial_xgb_model = xgb.XGBClassifier(**initial_model_params)\n","\n","# It's good practice to scale before training even this initial model for importance,\n","# for consistent preprocessing steps.\n","temp_scaler = StandardScaler()\n","X_train_scaled_temp = pd.DataFrame(temp_scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n","X_test_scaled_temp = pd.DataFrame(temp_scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n","X_train_scaled_temp = np.nan_to_num(X_train_scaled_temp)\n","X_test_scaled_temp = np.nan_to_num(X_test_scaled_temp)\n","\n","initial_xgb_model.fit(\n","    X_train_scaled_temp, y_train,\n","    eval_set=[(X_test_scaled_temp, y_test)],\n","    verbose=False\n",")\n","\n","# Get feature importances from the initial model\n","importances = initial_xgb_model.feature_importances_\n","feature_names = X_train.columns\n","importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n","importance_df = importance_df.sort_values('importance', ascending=False)\n","\n","print(\"\\nTop 10 Feature Importances from Initial Model:\")\n","print(importance_df.head(10))\n","\n","# Select features based on a threshold (e.g., median importance)\n","threshold = importance_df['importance'].median()\n","print(f\"\\nUsing a feature importance threshold of: {threshold:.4f}\")\n","\n","sfm = SelectFromModel(initial_xgb_model, threshold=threshold, prefit=True)\n","\n","X_train_selected_df = X_train.loc[:, sfm.get_support()]\n","X_test_selected_df = X_test.loc[:, sfm.get_support()]\n","\n","selected_features = X_train_selected_df.columns.tolist()\n","print(f\"\\nNumber of selected features: {len(selected_features)}\")\n","print(\"Selected Features:\", selected_features)\n","\n","# --- END NEW SECTION: Initial Model Training and Feature Selection ---\n","\n","# Scaling ONLY the selected features\n","scaler = StandardScaler()\n","X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_selected_df), columns=selected_features, index=X_train_selected_df.index)\n","X_test_scaled = pd.DataFrame(scaler.transform(X_test_selected_df), columns=selected_features, index=X_test_selected_df.index)\n","\n","# Handle NaNs after scaling\n","X_train_scaled = np.nan_to_num(X_train_scaled)\n","X_test_scaled = np.nan_to_num(X_test_scaled)\n","\n","print(f\"\\nDataset shape: {df.shape}\")\n","print(f\"Training set size (selected features): {X_train_scaled.shape}\")\n","print(f\"Test set size (selected features): {X_test_scaled.shape}\")\n","\n","# --- NEW SECTION: Hyperparameter Tuning (GridSearchCV) with Selected Features ---\n","print(\"\\n--- Starting Hyperparameter Tuning (GridSearchCV) on Selected Features ---\")\n","\n","# Define the parameter grid for GridSearchCV\n","# You can adjust these ranges based on previous insights or desired search space\n","param_grid = {\n","    # 'learning_rate': [0.05, 0.1],\n","    # 'max_depth': [5, 7],\n","    # 'subsample': [0.7, 0.9],\n","    # 'colsample_bytree': [0.5, 0.7, 0.9], # Explore this more for overfitting\n","    # 'reg_alpha': [0.1, 0.5, 1],       # L1 regularization\n","    # 'reg_lambda': [0.5, 1, 2],       # L2 regularization\n","    # 'gamma': [0.1, 0.2]\n","}\n","\n","# Initialize XGBClassifier with fixed parameters for GridSearchCV\n","# n_estimators should be set high enough to allow early stopping to find the optimum\n","# scale_pos_weight is crucial for imbalanced data, so include it here\n","fire_ratio = len(y_train[y_train == 1]) / len(y_train)\n","# Adjust scale_pos_weight if needed based on the resampled data's ratio\n","# In this case, your `rus` balances, so a specific scale_pos_weight might still be beneficial if it's not 1:1.\n","# Based on previous best, keeping 4.5.\n","fixed_params = {\n","    'objective': 'binary:logistic',\n","    'n_estimators': 1000, # Set a high number for early stopping\n","    'tree_method': 'hist',\n","    'enable_categorical': False,\n","    'use_label_encoder': False,\n","    'n_jobs': -1,\n","    'random_state': 42,\n","    'scale_pos_weight': 4.5 # Keep this as it's a critical imbalance parameter\n","}\n","\n","# For GridSearchCV, we often don't use early_stopping directly in the estimator,\n","# but rather through `callbacks` or let `fit` handle it if it supports it within CV,\n","# or simply choose a high `n_estimators` and rely on CV to pick robust parameters.\n","# For simplicity, we'll let GridSearchCV find the best params and then train with early stopping.\n","\n","xgb_base = xgb.XGBClassifier(**fixed_params)\n","\n","# Setup StratifiedKFold for cross-validation to maintain class balance\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","grid_search = GridSearchCV(\n","    estimator=xgb_base,\n","    param_grid=param_grid,\n","    scoring='roc_auc', # Optimize for ROC AUC\n","    cv=cv,\n","    verbose=2,\n","    n_jobs=-1\n",")\n","\n","grid_search.fit(X_train_scaled, y_train)\n","\n","print(\"\\n--- Hyperparameter Tuning Complete ---\")\n","print(f\"Best parameters found: {grid_search.best_params_}\")\n","print(f\"Best ROC AUC score: {grid_search.best_score_:.4f}\")\n","\n","best_params = grid_search.best_params_\n","\n","# --- END NEW SECTION: Hyperparameter Tuning (GridSearchCV) ---\n","\n","\n","# **XGBoost Model with Early Stopping and Evaluation History (TRAINING WITH BEST HYPERPARAMS)**\n","evals_result = {}\n","\n","# Combine fixed_params with best_params from GridSearchCV\n","final_model_params = {**fixed_params, **best_params}\n","# Ensure eval_metric is passed correctly\n","final_model_params['eval_metric'] = ['auc', 'aucpr', 'logloss']\n","final_model_params['early_stopping_rounds'] = 50 # Re-add early stopping for final fit\n","\n","model = xgb.XGBClassifier(**final_model_params)\n","\n","print(\"\\n--- Training final model with BEST selected features and BEST hyperparameters ---\")\n","model.fit(\n","    X_train_scaled, y_train,\n","    eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],\n","    verbose=1 # Keep verbose to see training progress\n",")\n","\n","evals_result = model.evals_result()\n","\n","# **Training Progress Visualization**\n","def plot_training_progress(evals_result):\n","    \"\"\"Plot training progress for all metrics\"\"\"\n","    eval_sets = list(evals_result.keys())\n","    if len(eval_sets) == 0:\n","        print(\"No evaluation results found. Make sure eval_set is provided during training.\")\n","        return\n","\n","    metrics = list(evals_result[eval_sets[0]].keys())\n","    n_metrics = len(metrics)\n","\n","    fig, axes = plt.subplots(1, n_metrics, figsize=(n_metrics * 6, 5))\n","    if n_metrics == 1:\n","        axes = [axes]\n","\n","    colors = ['blue', 'red']\n","    eval_names = ['Train', 'Test']\n","\n","    for i, metric in enumerate(metrics):\n","        ax = axes[i]\n","\n","        for j, eval_set in enumerate(eval_sets):\n","            if j < len(colors):\n","                scores = evals_result[eval_set][metric]\n","                epochs = range(1, len(scores) + 1)\n","                ax.plot(epochs, scores, label=f'{eval_names[j]} {metric.upper()}',\n","                        color=colors[j], linewidth=2, alpha=0.8)\n","\n","        ax.set_xlabel('Boosting Rounds (Iterations)', fontweight='bold')\n","        ax.set_ylabel(metric.upper(), fontweight='bold')\n","        ax.set_title(f'{metric.upper()} Progress', fontweight='bold')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","        if len(eval_sets) > 1:\n","            test_scores = evals_result[eval_sets[1]][metric]\n","            if metric == 'logloss':\n","                best_iter = np.argmin(test_scores)\n","            else:\n","                best_iter = np.argmax(test_scores)\n","            ax.axvline(x=best_iter+1, color='green', linestyle='--',\n","                       label=f'Best iteration: {best_iter+1}')\n","            ax.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Plot training progress\n","print(\"\\n=== Training Progress Visualization (Best Hyperparameters) ===\")\n","plot_training_progress(evals_result)\n","\n","\n","# **Additional Training Analysis**\n","def plot_detailed_training_analysis(evals_result):\n","    \"\"\"Plot detailed analysis of training progress\"\"\"\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    if 'auc' in evals_result['validation_0']:\n","        train_auc = evals_result['validation_0']['auc']\n","        test_auc = evals_result['validation_1']['auc']\n","        epochs = range(1, len(train_auc) + 1)\n","\n","        axes[0].plot(epochs, train_auc, label='Train AUC', color='blue', linewidth=2)\n","        axes[0].plot(epochs, test_auc, label='Test AUC', color='red', linewidth=2)\n","        axes[0].fill_between(epochs, train_auc, test_auc, alpha=0.2, color='gray')\n","        axes[0].set_xlabel('Boosting Rounds', fontweight='bold')\n","        axes[0].set_ylabel('AUC Score', fontweight='bold')\n","        axes[0].set_title('AUC Learning Curve', fontweight='bold')\n","        axes[0].legend()\n","        axes[0].grid(True, alpha=0.3)\n","\n","    if 'aucpr' in evals_result['validation_0']:\n","        train_prauc = evals_result['validation_0']['aucpr']\n","        test_prauc = evals_result['validation_1']['aucpr']\n","        epochs = range(1, len(train_prauc) + 1)\n","\n","        axes[1].plot(epochs, train_prauc, label='Train PR-AUC', color='blue', linewidth=2)\n","        axes[1].plot(epochs, test_prauc, label='Test PR-AUC', color='red', linewidth=2)\n","        axes[1].fill_between(epochs, train_prauc, test_prauc, alpha=0.2, color='gray')\n","        axes[1].set_xlabel('Boosting Rounds', fontweight='bold')\n","        axes[1].set_ylabel('PR-AUC Score', fontweight='bold')\n","        axes[1].set_title('Precision-Recall AUC Learning Curve', fontweight='bold')\n","        axes[1].legend()\n","        axes[1].grid(True, alpha=0.3)\n","\n","    if 'logloss' in evals_result['validation_0']:\n","        train_loss = evals_result['validation_0']['logloss']\n","        test_loss = evals_result['validation_1']['logloss']\n","        epochs = range(1, len(train_loss) + 1)\n","\n","        axes[2].plot(epochs, train_loss, label='Train Log Loss', color='blue', linewidth=2)\n","        axes[2].plot(epochs, test_loss, label='Test Log Loss', color='red', linewidth=2)\n","        axes[2].fill_between(epochs, train_loss, test_loss, alpha=0.2, color='gray')\n","        axes[2].set_xlabel('Boosting Rounds', fontweight='bold')\n","        axes[2].set_ylabel('Log Loss', fontweight='bold')\n","        axes[2].set_title('Log Loss Learning Curve', fontweight='bold')\n","        axes[2].legend()\n","        axes[2].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(\"\\n=== Overfitting Analysis (Best Hyperparameters) ===\")\n","    if 'auc' in evals_result['validation_0']:\n","        final_train_auc = evals_result['validation_0']['auc'][-1]\n","        final_test_auc = evals_result['validation_1']['auc'][-1]\n","        auc_gap = final_train_auc - final_test_auc\n","        print(f\"Final Train AUC: {final_train_auc:.4f}\")\n","        print(f\"Final Test AUC: {final_test_auc:.4f}\")\n","        print(f\"AUC Gap (Train - Test): {auc_gap:.4f}\")\n","\n","        if auc_gap > 0.05:\n","            print(\"‚ö†Ô∏è  Potential overfitting detected (AUC gap > 0.05)\")\n","        else:\n","            print(\"‚úÖ No significant overfitting detected\")\n","\n","    print(f\"\\n=== Early Stopping Analysis (Best Hyperparameters) ===\")\n","    print(f\"Total boosting rounds completed: {model.n_estimators}\")\n","    print(f\"Best iteration: {model.best_iteration}\")\n","    print(f\"Best score: {model.best_score:.4f}\")\n","\n","# Plot detailed training analysis\n","plot_detailed_training_analysis(evals_result)\n","\n","# Set feature names explicitly for the final model (using selected features)\n","model.get_booster().feature_names = selected_features\n","print(\"Feature names set in model (after selection):\", model.get_booster().feature_names)\n","\n","# **Predictions**\n","y_pred = model.predict(X_test_scaled)\n","y_proba = model.predict_proba(X_test_scaled)[:, 1]\n","\n","# Find optimal threshold using precision-recall tradeoff\n","precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n","f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n","optimal_idx = np.argmax(f1_scores)\n","optimal_threshold = thresholds[optimal_idx]\n","y_pred_optimized = ((y_proba >= optimal_threshold)).astype(int)\n","print(f\"\\nOptimal threshold (based on F1-score optimization): {optimal_threshold:.4f}\")\n","\n","# **Evaluation Metrics**\n","print(\"\\n=== Model Performance Metrics (Best Hyperparameters) ===\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred_optimized))\n","print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n","print(f\"R¬≤ Score: {r2_score(y_test, y_proba):.4f}\")\n","mae = mean_absolute_error(y_test, y_pred_optimized)\n","mse = mean_squared_error(y_test, y_pred_optimized)\n","rmse = np.sqrt(mse)\n","print(f\"MAE: {mae:.4f}\")\n","print(f\"MSE: {mse:.4f}\")\n","print(f\"RMSE: {rmse:.4f}\")\n","\n","# **Visualizations**\n","\n","## Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred_optimized)\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n","plt.title(\"Confusion Matrix (Best Hyperparameters)\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n","\n","## ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, y_proba)\n","roc_auc = auc(fpr, tpr)\n","plt.figure(figsize=(6, 4))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate', fontweight='bold')\n","plt.ylabel('True Positive Rate', fontweight='bold')\n","plt.title('ROC Curve (Best Hyperparameters)', fontweight='bold')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","## Precision-Recall Curve\n","precision, recall, _ = precision_recall_curve(y_test, y_proba)\n","plt.figure(figsize=(6, 4))\n","plt.plot(recall, precision, color='b', lw=2)\n","plt.xlabel('Recall', fontweight='bold')\n","plt.ylabel('Precision', fontweight='bold')\n","plt.title('Precision-Recall Curve (Best Hyperparameters)', fontweight='bold')\n","plt.show()\n","\n","## Calibration Curve\n","prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)\n","plt.figure(figsize=(6, 4))\n","plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel('Mean Predicted Probability', fontweight='bold')\n","plt.ylabel('Fraction of Positives', fontweight='bold')\n","plt.title('Calibration Curve (Best Hyperparameters)', fontweight='bold')\n","plt.legend()\n","plt.show()\n","\n","## Feature Importance (for the final model with selected features)\n","importances_final = model.feature_importances_\n","feature_names_final = selected_features\n","importance_df_final = pd.DataFrame({'feature': feature_names_final, 'importance': importances_final})\n","importance_df_final = importance_df_final.sort_values('importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='importance', y='feature', data=importance_df_final.head(len(selected_features)))\n","plt.title('Feature Importances (Final Model with Best Hyperparameters)', fontweight='bold')\n","plt.xticks(fontweight='bold')\n","plt.yticks(fontweight='bold')\n","plt.xlabel('Importance', fontweight='bold')\n","plt.ylabel('Feature', fontweight='bold')\n","plt.show()\n","\n","\n","# **Training Summary**\n","print(\"\\n=== Training Summary (Best Hyperparameters) ===\")\n","print(f\"‚úÖ Model training completed successfully with selected features and best hyperparameters!\")\n","print(f\"üìä Best iteration: {model.best_iteration}\")\n","print(f\"üéØ Best validation score: {model.best_score:.4f}\")\n","print(f\"üîÑ Total boosting rounds: {model.n_estimators}\")\n","print(f\"‚è±Ô∏è  Early stopping triggered at round: {model.best_iteration}\")\n","\n","# Save the model and scaler (updated for selected features and best hyperparameters)\n","# import pickle\n","# with open('xgboost_model_final.pkl', 'wb') as f:\n","#     pickle.dump(model, f)\n","# with open('scaler_final.pkl', 'wb') as f:\n","#     pickle.dump(scaler, f)"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# --- Your Model Performance Data ---\n","# Extracting the scores directly from your provided output\n","train_auc = 0.9995\n","test_auc = 0.9432\n","\n","# From Classification Report (macro avg for balanced view across classes)\n","accuracy = 0.88\n","macro_precision = 0.88\n","macro_recall = 0.88\n","macro_f1 = 0.88\n","\n","# Other metrics\n","roc_auc_score_val = 0.9430 # This is essentially the same as test_auc, will use test_auc for consistency\n","r2_score_val = 0.5971\n","mae = 0.1211\n","mse = 0.1211\n","rmse = 0.3480\n","\n","# --- Create DataFrames for Plotting ---\n","\n","# 1. Classification Metrics\n","classification_metrics = {\n","    'Metric': ['Train AUC', 'Test AUC', 'Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1-Score'],\n","    'Score': [train_auc, test_auc, accuracy, macro_precision, macro_recall, macro_f1]\n","}\n","df_classification = pd.DataFrame(classification_metrics)\n","\n","# 2. Regression/Error Metrics\n","regression_metrics = {\n","    'Metric': ['R¬≤ Score', 'MAE', 'MSE', 'RMSE'],\n","    'Score': [r2_score_val, mae, mse, rmse]\n","}\n","df_regression = pd.DataFrame(regression_metrics)\n","\n","# --- Plotting the Bar Charts ---\n","\n","# Plot 1: Classification Metrics\n","plt.figure(figsize=(12, 7))\n","sns.barplot(x='Score', y='Metric', data=df_classification, palette='viridis')\n","plt.xlim(0, 1.0) # Classification metrics are typically between 0 and 1\n","plt.title('Key Classification Performance Metrics', fontsize=16, fontweight='bold')\n","plt.xlabel('Score', fontsize=12, fontweight='bold')\n","plt.ylabel('Metric', fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","\n","# Add score values on the bars\n","for index, row in df_classification.iterrows():\n","    plt.text(row.Score + 0.01, index, f'{row.Score:.4f}', color='black', ha=\"left\", va='center', fontsize=10)\n","\n","plt.grid(axis='x', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","# Plot 2: Regression/Error Metrics\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Score', y='Metric', data=df_regression, palette='magma')\n","plt.title('Regression/Error Performance Metrics', fontsize=16, fontweight='bold')\n","plt.xlabel('Score', fontsize=12, fontweight='bold')\n","plt.ylabel('Metric', fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","\n","# Add score values on the bars\n","for index, row in df_regression.iterrows():\n","    plt.text(row.Score + (0.01 if row.Score < 0.5 else -0.05), index, f'{row.Score:.4f}',\n","             color='black' if row.Score < 0.5 else 'white', ha=\"left\" if row.Score < 0.5 else \"right\", va='center', fontsize=10)\n","\n","plt.grid(axis='x', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"vuO9zrbAta7X"},"execution_count":null,"outputs":[]}]}